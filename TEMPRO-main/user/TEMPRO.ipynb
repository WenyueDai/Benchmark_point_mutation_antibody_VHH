{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4f4304",
   "metadata": {},
   "source": [
    "# TEMPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f4589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import math\n",
    "\n",
    "### Data Wrangling and Plots\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bea643",
   "metadata": {},
   "source": [
    "## 1) Load the ESM Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999671a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### note: loading the ESM 15 Billion parameters takes longer\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "# esm.pretrained.esm2_t48_15B_UR50D() \n",
    "# esm.pretrained.esm2_t36_3B_UR50D() \n",
    "# esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "#model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "#model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e68061",
   "metadata": {},
   "source": [
    "## 2) Load fasta files (it can include single to mulitple sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0d22a",
   "metadata": {},
   "source": [
    "### 2a) declare fasta function and input fasta filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a15a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('>4IDL_1|Chain A|Single Domain Antibody VHH A9|Lama glama (9844)',\n",
       "  'MAKVQLQQSGGGAVQTGGSLKLTCLASGNTASIRAMGWYRRAPGKQREWVASLTTTGTADYGDFVKGRFTISRDNANNAATLQMDSLKPEDTAVYYCNADGRRFDGARWREYESWGQGTQVTISSAAALEHHHHHH'),\n",
       " ('>4TYU_1|Chains A, B|Single Domain Antibody|Lama glama (9844)',\n",
       "  'GSHMEVQLVESGGGLVQAGDSLRLSCTASGRTFSRAVMGWFRQAPGKEREFVAAISAAPGTAYYAFYADSVRGRFSISADSAKNTVYLQMNSLKPEDTAVYYCAADLKMQVAAYMNQRSVDYWGQGTQVTVSS'),\n",
       " ('>4U05_1|Chains A, B|Single Domain Antibody|Lama glama (9844)',\n",
       "  'GSHMEVQLVESGGGLVQAGDSLRLSCTASGRTFSRAVMGWFRQAPGKEREFVAAISAAPGTAYYAFYADSVRGRFSIAADSAKNTVYLQMNSLKPEDTAVYYCAADLKMQVAAYMNQRSVDYWGQGTQVTVSS'),\n",
       " ('>4W68_1|Chains A, B|Single domain antibody|Lama glama (9844)',\n",
       "  'GSHMEVQLVESGGGLVQAGDSLRLSATASGRTFSRAVMGWFRQAPGKEREFVAAISAAPGTAYYAFYADSVRGRFSISADSAKNTVYLQMNSLKPEDTAVYYVAADLKMQVAAYMNQRSVDYWGQGTQVTVSS'),\n",
       " ('>4W70_1|Chain A|Single Domain Antibody|Lama glama (9844)',\n",
       "  'MAEVQLVESGGGLVQAGDSLRLSATASGRTFSRAVMGWFRQAPGKEREFVAAISAAPGTAYYAFYADSVRGRFSISADSAKNTVYLQMNSLKPEDTAVYYVAADLKMQVAAYMNQRSVDYWGQGTQVTVSSAAALEHHHHHH'),\n",
       " ('>5SV3_1|Chains A, C|Anti-Ricin A-chain Single Domain Antibody (sdAb) A3C8|Lama glama (9844)',\n",
       "  'MAEVQLVESGGGLVQAGDSLRLSCTASGRTLGDYGVAWFRQAPGKEREFVSVISRSTIITDYADSVRGRFSISADSAKNTVYLQMNSLKPEDTAVYYCAVIANPVYATSRNSDDYGHWGQGTQVTVSSAAALEHHHHHH')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_fasta(fp):\n",
    "        name, seq = None, []\n",
    "        for line in fp:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if name: yield (name, ''.join(seq))\n",
    "                name, seq = line, []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "        if name: yield (name, ''.join(seq))\n",
    "\n",
    "# input your fasta\n",
    "#fasta_filename = 'sdabs.fasta'\n",
    "#fasta_filename = 'sample.fasta'\n",
    "fasta_filename = 'experimentals.fasta'\n",
    "\n",
    "data = []\n",
    "with open(fasta_filename) as fp:\n",
    "    for name, seq in read_fasta(fp):\n",
    "        data.append((name, seq))\n",
    "        \n",
    "data2 = data[0:9150] # Making sure the dataset being looked at is divisible by chunk/batch size\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b41370",
   "metadata": {},
   "source": [
    "### 2b) extract their ESM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2d767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "sequence_representations_list = []\n",
    "chunk_size = 25\n",
    "for i in range(0, len(data2), chunk_size):\n",
    "    chunk = data2[i:i+chunk_size]\n",
    "    print(i+chunk_size)\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(chunk)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "    #    results = model(batch_tokens, repr_layers=[48], return_contacts=True) # 15B\n",
    "    #token_representations = results[\"representations\"][48]\n",
    "    #    results = model(batch_tokens, repr_layers=[36], return_contacts=True) # 3B\n",
    "    #token_representations = results[\"representations\"][36]\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True) # 650M\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    \n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "    sequence_representations_list.append(sequence_representations) # torch.stack(sequence_representations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1df7bd",
   "metadata": {},
   "source": [
    "### 2c) assign as X for model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa468189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05619194, -0.00791028, -0.0454305 , ..., -0.1043366 ,\n",
       "        -0.01007742,  0.08740339],\n",
       "       [-0.07241704,  0.01124016, -0.0123387 , ..., -0.07190275,\n",
       "        -0.03326014,  0.14270751],\n",
       "       [-0.07218102,  0.01011745, -0.013458  , ..., -0.07263383,\n",
       "        -0.03575287,  0.14528897],\n",
       "       [-0.06507177,  0.00978963, -0.01847906, ..., -0.05832915,\n",
       "        -0.02844086,  0.13671044],\n",
       "       [-0.0672899 ,  0.00924716, -0.0324568 , ..., -0.0796625 ,\n",
       "        -0.01490547,  0.10339514],\n",
       "       [-0.07008886, -0.00545676, -0.02323345, ..., -0.09700638,\n",
       "        -0.00512653,  0.08155268]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list = [item for sublist in sequence_representations_list for item in sublist]\n",
    "X = torch.stack(flat_list, dim=0).cpu().detach().numpy()\n",
    "X\n",
    "### think twice before saving\n",
    "#np.savetxt(\"nanobody_embeddings.csv\", X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573d31c",
   "metadata": {},
   "source": [
    "## 3) Load your model of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ec4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_650M.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_3B.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_15B.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10db964",
   "metadata": {},
   "source": [
    "## 4) Predict Tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd40cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[59.854244],\n",
       "       [75.07941 ],\n",
       "       [75.905106],\n",
       "       [60.497295],\n",
       "       [67.250626],\n",
       "       [61.582012]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb6f23",
   "metadata": {},
   "source": [
    "### excerpt from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa8526e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "5.35292833981548 7.208072276728451 0.5359914055902835\n"
     ]
    }
   ],
   "source": [
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/pchars_maestro.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/af2.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/nsp3.keras\")\n",
    "model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_650M.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_3B.keras\")\n",
    "#model = keras.models.load_model(\"./saved_ANNmodels_1500epoch/ESM_15B.keras\")\n",
    "\n",
    "#data = pd.read_csv(\"./tm_predictors/tm_dataset_nsp.csv\")\n",
    "#data = pd.read_csv(\"./tm_predictors/tm_dataset_af2.csv\")\n",
    "#data = pd.read_csv(\"./tm_predictors/tm_dataset_pchars_maestro.csv\")\n",
    "data = pd.read_csv(\"./tm_predictors/tm_dataset_ESM_650M.csv\", header=None)\n",
    "#data = pd.read_csv(\"./tm_predictors/tm_dataset_ESM_3B.csv\", header=None)\n",
    "#data = pd.read_csv(\"./tm_predictors/tm_dataset_ESM_15B.csv\", header=None)\n",
    "\n",
    "y = pd.read_excel(\"./tm_predictors/sdab_data.xlsx\")\n",
    "y = y.tm\n",
    "x = data\n",
    "\n",
    "# assign 80:20 ratio\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mae = mean_absolute_error(y_test, model.predict(x_test))\n",
    "rmse = math.sqrt(mean_squared_error(y_test, model.predict(x_test)))\n",
    "coeff_det=r2_score(y_test, model.predict(x_test))\n",
    "\n",
    "print(mae, rmse, coeff_det)\n",
    "\n",
    "# results should be the same as the paper: mae=4.03, rmse=5.66, r2=0.71\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841df75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
